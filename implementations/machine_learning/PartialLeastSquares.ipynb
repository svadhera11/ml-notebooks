{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Partial Least Squares (PLS) differs from Principal Components Regression (PCR) mainly in the manner that the orthogonal directions are chosen. Instead of an unsupervised manner, using the variance of the data as a guide, PLS **also** takes into account the output labels $\\textbf{y}$.\n",
    "\n",
    "In this notebook, we shall implement Partial Least Squares from scratch and apply it to a synthetically-generated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "\n",
    "# bad practice in general, but useful to declutter output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PLS algorithm goes as follows:\n",
    "\n",
    "**Step 1**: Standardize each feature ($\\textbf{x}_j$) to have mean $0$ and variance $1$. Also standardize the output labels $\\textbf{y}$ the same way.\n",
    "\n",
    "**Step 2**: Initialize the algorithm with $\\hat{\\textbf{y}}^{(0)} = \\text{mean}(y)$ and $\\textbf{x}_j^{(0)} = \\textbf{x}_j$ for all $p$ features ($j = 1, ..., p$).\n",
    "\n",
    "**Step 3**: Loop over the following steps $p$ times ($m = 1, ..., p$):\n",
    "\n",
    "**Step 3a**: Define $\\phi_{m, j} = \\textbf{x}_{j}^{(m-1)}\\cdot \\textbf{y}$ for all $j$. This is the projection of $\\textbf{y}$ on (orthogonalized) feature $\\textbf{x}^{(m-1)}_{j}$. See step 3e for more details.\n",
    "\n",
    "**Step 3b**: Define $\\textbf{z}_{m} = \\sum_{j=1}^{p} \\phi_{m, j}\\textbf{x}_{j}^{(m-1)}$. This is the $m^{\\text{th}}$ \"derived direction\".\n",
    "\n",
    "**Step 3c**: Now that we have the derived direction, perform univariate regression to find out its corresponding coefficient: $$ \\hat{\\theta}_{m} = \\frac{\\textbf{z}_{m}\\cdot\\textbf{y}}{|| \\textbf{z}_{m} ||^2}$$\n",
    "We can do this sort of direct projection because the derived directions are orthogonal to each other.\n",
    "\n",
    "**Step 3d**: Next, update the prediction, $\\hat{y}^{(m)} = \\hat{y}^{(m-1)} + \\hat{\\theta}_{m}\\textbf{z}_{m}$\n",
    "\n",
    "**Step 3e**: Finally, before moving on to the next step, remove the contribution of the $m^{\\text{th}}$ derived direction by orthogonalizing the input features. This naturally ensures that all of the derived directions are orthogonal to each other: $$ \\textbf{x}_{j}^{(m)} = \\textbf{x}_{j}^{(m-1)} - \\left(\\frac{\\textbf{z}_{m} \\cdot \\textbf{x}_{j}^{(m-1)} }{|| \\textbf{z}_{m} ||^2}\\right)\\textbf{z}_{m}$$\n",
    "\n",
    "\n",
    "Finally, note that at the end of the day, $\\textbf{z}_{m}$ and $\\textbf{x}_{j}$ are linearly related to each other. When the whole procedure is done, we can invert the relationship between them to get the coefficients of the model in terms of the original features, $\\textbf{x}_{j}$. We illustrate this in the \"Implementation\" section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLS():\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "\n",
    "        # X @ beta.T = yhat\n",
    "        self._beta = None \n",
    "\n",
    "        # parameters for standardization, learnt from the training set during the `fit` call.\n",
    "        # used during the `predict` call as well.\n",
    "\n",
    "        self._mu_X  = None\n",
    "        self._mu_y  = None\n",
    "        self._std_X  = None\n",
    "        self._std_y  = None\n",
    "\n",
    "        # number of ignored features\n",
    "        self._num_ignored_features = None\n",
    "\n",
    "        return None\n",
    "    \n",
    "    def _compute_statistics(self, X, y):\n",
    "        \n",
    "        self._mu_X  = np.mean(X, axis=0)\n",
    "        self._mu_y  = np.mean(y)\n",
    "        self._std_X  = np.std(X, axis=0)\n",
    "        self._std_y  = np.std(y, axis=0)\n",
    "\n",
    "        return None\n",
    "\n",
    "    \n",
    "    def _standardize(self, X, y):\n",
    "        \n",
    "        X_standardized = (X - self._mu_X)/self._std_X\n",
    "        y_standardized = (y - self._mu_y)/self._std_y\n",
    "        \n",
    "        return X_standardized, y_standardized\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "\n",
    "        self._compute_statistics(X, y)\n",
    "        X0, y0 = self._standardize(X, y)\n",
    "\n",
    "        N, p = X0.shape\n",
    "        \n",
    "        # formally the mean of y, but note that after standardization that mean is 0\n",
    "        yhat = np.zeros((N, 1))\n",
    "        \n",
    "        Z = np.zeros((N, p))\n",
    "        theta = np.zeros((1, p))\n",
    "        \n",
    "        # \"forward\" pass - computing the derived directions and associated parameters, theta.\n",
    "\n",
    "        num_ignored_features : int = 0 # keep track of ignored features.\n",
    "\n",
    "        for m in range(p):\n",
    "            \n",
    "            z_m = np.zeros((N, 1))\n",
    "\n",
    "            for j in range(p):\n",
    "                x_j = X0[:, j].reshape((N, 1))\n",
    "                phi_m_j = np.dot(x_j.T, y0)\n",
    "                z_m = z_m + phi_m_j*x_j\n",
    "\n",
    "            if (np.linalg.norm(z_m) >= 1e-10): # drop any components that are \"too small\"\n",
    "\n",
    "                z_m = z_m/np.linalg.norm(z_m) # unit norm (good practice)\n",
    "                theta_m = np.dot(z_m.T, y0)/np.dot(z_m.T, z_m)\n",
    "\n",
    "                theta[0, m] = theta_m\n",
    "\n",
    "                yhat = yhat + (theta_m * z_m)\n",
    "\n",
    "                for j in range(p):\n",
    "                    x_j = X0[:, j].reshape((N, 1))\n",
    "                    x_j = x_j - ((np.dot(z_m.T, x_j)/np.dot(z_m.T, z_m))*z_m)\n",
    "                    X0[:, j] = x_j.reshape(-1)\n",
    "                \n",
    "                Z[:, m] = z_m.reshape(-1)\n",
    "            else:\n",
    "                num_ignored_features += 1\n",
    "                \n",
    "        self._num_ignored_features = num_ignored_features\n",
    "\n",
    "        # \"backward\" pass - recovering parameters of X (beta) from Z (and theta)\n",
    "        # X@A = Z, yhat = Z @ theta.T => yhat = X @ (A@theta.T) = X @ (beta.T)\n",
    "        # use pinv() (pseudoinverse) to deal with the case when X is not invertible or has high condition number.\n",
    "\n",
    "        A = np.matmul(np.linalg.pinv(X), Z)\n",
    "        beta = np.matmul(A, theta.T).T\n",
    "        \n",
    "        self._beta = beta\n",
    "\n",
    "        return None\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        X0 = (X - self._mu_X)/self._std_X # standardize the features.\n",
    "        yhat0 = np.matmul(X0,  self._beta.T) # \"standardized\" prediction.\n",
    "        yhat = (yhat0 * self._std_y) + self._mu_y # rescaling to scale of training data.\n",
    "\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions - mainly used for generating helpful information and performance summaries\n",
    "# Also used for working with Ordinary least squares solution by Normal equations method.\n",
    "\n",
    "def generate_data(n_samples : int, n_features : int, collinear : bool = False, n_collinear : int = 0, corr_strength : float = 0.9, noise : float = 1.0):\n",
    "    '''\n",
    "    Wrapper to generate data for regression. Same as `LinearRegression.ipynb`\n",
    "    '''\n",
    "    X, y, coef = sklearn.datasets.make_regression(n_samples = n_samples, n_features=n_features,\n",
    "                                 n_informative=n_features - (collinear*n_collinear), n_targets=1, \n",
    "                                 bias=2.0, effective_rank=n_features - (collinear*n_collinear),\n",
    "                                 noise=noise, shuffle=True, random_state=42, coef=True)\n",
    "    y : npt.NDArray[np.float64] = y.reshape(-1, 1)\n",
    "    \n",
    "    if (collinear==True):\n",
    "        for i in range(n_features - n_collinear, n_features):\n",
    "            base_feature = np.random.randint(0, n_features - n_collinear)\n",
    "            X[:, i] = corr_strength * X[:, base_feature] + (1 - corr_strength) * np.random.randn(n_samples) * noise\n",
    "            \n",
    "    return X, y, coef\n",
    "\n",
    "def MaxVif(X : npt.NDArray[np.float64]) -> npt.NDArray[np.float64]:\n",
    "    ''' Returns the Maximum VIF amongst all the features'''\n",
    "    N, K = X.shape\n",
    "    vif_i : list[float] = []\n",
    "    for i in range(K):\n",
    "        x_i : npt.NDArray[np.float64] = X[:, i].reshape(-1)\n",
    "        x_rest : npt.NDArray[np.float64] = np.delete(X, i, axis=1)\n",
    "        x_i_pred : npt.NDArray[np.float64] = sklearn.linear_model.LinearRegression().fit(x_rest, x_i).predict(x_rest)\n",
    "        R_i_sq : float = 1 - np.sum(np.power(x_i - x_i_pred, 2.0))/(np.sum(np.power(x_i - np.mean(x_i), 2)))\n",
    "        vif_i.append(1.0/(1.0 - R_i_sq))\n",
    "    vif_i = np.array(vif_i)\n",
    "\n",
    "    return np.array([np.max(vif_i), np.argmax(vif_i)])\n",
    "\n",
    "def PerformanceSummary(y : npt.NDArray[np.float64], y_pred : npt.NDArray[np.float64]) -> dict[str, float]:\n",
    "    y_bar : float = np.mean(y)\n",
    "    mse_f : float = np.sum(np.power(y - y_pred, 2.0))/len(y)\n",
    "    mae_f : float = np.sum(np.absolute(y - y_pred))/len(y)\n",
    "    rsq : float = 1 - (np.sum(np.power((y - y_pred), 2.0)))/(np.sum(np.power((y - y_bar), 2.0)))\n",
    "    perf : dict[str, float] = {\"MSE\":mse_f, \"MAE\": mae_f, \"R^2\": rsq}\n",
    "\n",
    "    return perf\n",
    "\n",
    "def generate_datasets(n_samples : int , n_train : int, n_features : int = 10, collinear : bool = True, n_collinear : int = 2, corr_strength : float = 0.6, noise : float = 2.0) -> tuple[npt.NDArray[np.float64], npt.NDArray[np.float64], npt.NDArray[np.float64], npt.NDArray[np.float64]]:\n",
    "\n",
    "    def standardize(X : npt.NDArray[np.float64], y : npt.NDArray[np.float64], train_set : bool = False, params : tuple[npt.NDArray[np.float64], npt.NDArray[np.float64], npt.NDArray[np.float64], npt.NDArray[np.float64]] = None) -> tuple[npt.NDArray[np.float64], npt.NDArray[np.float64], tuple[npt.NDArray[np.float64], npt.NDArray[np.float64], npt.NDArray[np.float64], npt.NDArray[np.float64]]]:\n",
    "        '''Standardize Dataset - a function brought over for ordinary linear regression. Defined within `generate_datasets` to avoid confusion and conflict with PLS class.'''\n",
    "        if (train_set == True) and (params is None):\n",
    "            mu_X : npt.NDArray[np.float64] = np.mean(X, axis=0)\n",
    "            mu_y : npt.NDArray[np.float64] = np.mean(y)\n",
    "            std_X : npt.NDArray[np.float64] = np.std(X, axis=0)\n",
    "            std_y : npt.NDArray[np.float64] = np.std(y, axis=0)\n",
    "\n",
    "            X : npt.NDArray[np.float64] = (X - mu_X)/std_X\n",
    "            y : npt.NDArray[np.float64] = (y - mu_y)/std_y\n",
    "            params : tuple[npt.NDArray[np.float64], npt.NDArray[np.float64], npt.NDArray[np.float64], npt.NDArray[np.float64]] = (mu_X, mu_y, std_X, std_y)\n",
    "            \n",
    "        elif (train_set == False) and (params is not None):\n",
    "            mu_X, mu_y, std_X, std_y = params\n",
    "            X : npt.NDArray[np.float64] = (X - mu_X)/std_X\n",
    "            y : npt.NDArray[np.float64] = (y - mu_y)/std_y\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid set of inputs! Please ensure `params` is not None for train_set == False\")\n",
    "        \n",
    "        return X, y, params\n",
    "\n",
    "    X, y, _ = generate_data(n_samples, n_features = n_features, collinear=collinear, n_collinear=n_collinear, corr_strength=corr_strength, noise = noise)\n",
    "\n",
    "    X_train : npt.NDArray[np.float64] = X[:n_train]\n",
    "    y_train : npt.NDArray[np.float64] = y[:n_train]\n",
    "    X_train, y_train, params = standardize(X_train, y_train, train_set = True, params=None)\n",
    "\n",
    "    X_test : npt.NDArray[np.float64] = X[n_train:]\n",
    "    y_test : npt.NDArray[np.float64] = y[n_train:]\n",
    "    X_test, y_test, params = standardize(X_test, y_test, train_set = False, params=params)\n",
    "\n",
    "    max_vif, max_vif_idx = MaxVif(X_train)\n",
    "\n",
    "    if (max_vif < 5):\n",
    "        print(f\"Max VIF: {max_vif.round(2)} at column: {int(max_vif_idx)}\")\n",
    "        print(\"Maximum VIF in training set < 5, no need to deal with multicollinearity\")\n",
    "    else:\n",
    "        print(\"WARNING!\")\n",
    "        print(f\"Max VIF: {max_vif.round(2)} at column: {int(max_vif_idx)}\")\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def NormalEquationSolution(X_train: npt.NDArray[np.float64], y_train : npt.NDArray[np.float64], X_test : npt.NDArray[np.float64], y_test : npt.NDArray[np.float64]):\n",
    "\n",
    "    def f(x: npt.NDArray[np.float64], w: npt.NDArray[np.float64], b: float) -> float:\n",
    "        ''' Linear Regression equation - local function '''\n",
    "        f_wb: float = np.dot(w, x) + b\n",
    "        return f_wb\n",
    "\n",
    "    def normal_solution(X: npt.NDArray[np.float64], y: npt.NDArray[np.float64]) -> npt.NDArray[np.float64]:\n",
    "        '''Find solution of regression by normal equations - local function'''\n",
    "        beta : npt.NDArray[np.float64] = np.matmul(np.matmul(np.linalg.inv(np.matmul(X.T, X)), X.T), y)\n",
    "        return beta\n",
    "    \n",
    "    w_fit : npt.NDArray[np.float64] = normal_solution(X_train, y_train).reshape(-1)\n",
    "    b_fit : float = 0.0\n",
    "    # bias is zero after standardization, no need to fit it.\n",
    "    \n",
    "    # performance metrics\n",
    "    y_pred : npt.NDArray[np.float64] = np.zeros(y_test.shape)\n",
    "    for i in range(len(y_pred)):\n",
    "        y_pred[i] = f(X_test[i], w_fit, b_fit)\n",
    "\n",
    "    perf : dict[str, float] = PerformanceSummary(y_test, y_pred)\n",
    "    \n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(\"Solution based on Normal Equations (Ordinary Least Squares)\")\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(f\"MSE after training (test set): {perf['MSE'].round(3)}\")\n",
    "    print(f\"MAE after training (test set): {perf['MAE'].round(3)}\")\n",
    "    print(f\"R^2 after training (test set): {perf['R^2'].round(3)}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "def PartialLeastSquaresSolution(X_train: npt.NDArray[np.float64], y_train : npt.NDArray[np.float64], X_test : npt.NDArray[np.float64], y_test : npt.NDArray[np.float64]):\n",
    "\n",
    "    PLSRegressor = PLS()\n",
    "    PLSRegressor.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = PLSRegressor.predict(X_test)\n",
    "    perf : dict[str, float] = PerformanceSummary(y_test, y_pred)\n",
    "\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(\"Solution based on Partial Least Squares\")\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(f\"PLS dropped {PLSRegressor._num_ignored_features} features of {X_train.shape[-1]} features.\")\n",
    "    print(f\"MSE after training (test set): {perf['MSE'].round(3)}\")\n",
    "    print(f\"MAE after training (test set): {perf['MAE'].round(3)}\")\n",
    "    print(f\"R^2 after training (test set): {perf['R^2'].round(3)}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING!\n",
      "Max VIF: inf at column: 0\n",
      "--------------------------------------------------------\n",
      "Solution based on Normal Equations (Ordinary Least Squares)\n",
      "--------------------------------------------------------\n",
      "MSE after training (test set): 91409.647\n",
      "MAE after training (test set): 266.938\n",
      "R^2 after training (test set): -76426.371\n",
      "--------------------------------------------------------\n",
      "Solution based on Partial Least Squares\n",
      "--------------------------------------------------------\n",
      "PLS dropped 180 features of 200 features.\n",
      "MSE after training (test set): 1.114\n",
      "MAE after training (test set): 0.861\n",
      "R^2 after training (test set): 0.069\n"
     ]
    }
   ],
   "source": [
    "n_samples = 100\n",
    "n_train = int(0.8 * n_samples)\n",
    "n_features = 200\n",
    "n_collinear = 5\n",
    "\n",
    "\n",
    "X_train, y_train, X_test, y_test = generate_datasets(n_samples, n_train, n_features = n_features,\n",
    "                                                      collinear = True, n_collinear = n_collinear, corr_strength = 0.4, \n",
    "                                                      noise = 0.1)\n",
    "\n",
    "NormalEquationSolution(X_train, y_train, X_test, y_test)\n",
    "PartialLeastSquaresSolution(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion and Closing Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this notebook, we specifically worked with Partial least squares (PLS) and saw how it uses information about the labels to inform the choice of \"derived directions\". This can sometimes lead to better results than PCR, since \"explained variance\" is not always a strong predictor of variable importance by itself.\n",
    "- An illustration showed us PLS working in one of the primary areas it shines - wide data. PLS is also useful as a dimensionality reduction technique. One can simply reduce the dimensions by dropping some $z_m$. It is also very useful with highly collinear data.\n",
    "- It should be noted that Partial Least Squares also runs the risk of being prohibitively computationally expensive, much like PCR, even after all the loops in this version are vectorized."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
