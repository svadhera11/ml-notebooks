{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Components Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we implement Principal Components Regression (PCR) from scratch and apply it to a synthetically generated dataset. We will also see the advantages of Principal Components Regression versus ordinary Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, before we begin anything, we import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "\n",
    "# bad practice in general, but useful to declutter output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The main idea behind Principal Components Regression (PCR) and its \"successor\", Partial Least Squares (PLS) is that of finding new directions (or new features) from the given vectors (features). The goal is to find features that would better represent the data, and hence, help us make better predictions.\n",
    "\n",
    "It is based on a method called Principal Components Analysis (PCA), which is primarily a dimensionality reduction technique that works by finding directions (features) where the data has the maximum variance. This helps us pick and choose the \"most important\" features for the data and drop the remaining ones.\n",
    "\n",
    "- Note: in many cases, the feature along which the data has the largest variance is the most important one in terms of predictive power. This, however, is not a strict condition and there are times when it is violated.\n",
    "\n",
    "Moreover, the directions found by PCA are **mutually orthogonal**, and hence PCA has the added advantage of finding directions or features that are **linearly independent** - fixing any collinearity or linear dependence present in the original features. \n",
    "\n",
    "These are the key reasons why we are interested in PCR - not only can we perform dimensionality reduction on our feature set, we can also effectively deal with collinearity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "Let $X$ be the $N \\times p$ input matrix. Perform the **Singular Value Decomposition** (SVD) of $X$ as follows:\n",
    "\n",
    "$$X = U D V^T$$\n",
    "\n",
    "Where $U$ is a $N \\times p$ orthogonal matrix, $D$ is a $p \\times p$ diagonal matrix and $V$ is a $p \\times p$ orthogonal matrix. Here $N$ is the number of examples and $p$ is the number of features per example.\n",
    "\n",
    "The columns of $V$ are called the **Principal Component Directions** of $X$. The transformed dataset, $Z$, can be computed by projecting the dataset $X$ onto the principal component directions as follows:\n",
    "\n",
    "$$ Z = X V$$\n",
    "\n",
    "Now all we need to do is treat $Z$ as the dataset and perform regression with the same labels, $y$. For any future vectors (say, some vector $x$), all we have to do, again, is to project them as:\n",
    "\n",
    "$$ z = x V $$\n",
    "\n",
    "to get the transformed vector.\n",
    "\n",
    "Note that this is assuming we don't do any dimensionality reduction - we could (and in many cases, do), of course, perform trivial dimensionality reduction by truncating the number of columns of $Z$ to whatever we desire, since the columns of $V$ are ordered in terms of decreasing importance when performing SVD (that is, decreasing corresponding singular values).\n",
    "\n",
    "\n",
    "**NOTES** \n",
    "- The inputs $X$ (and the vector $x$) are assumed to be standardized. Most often in practice the labels $y$ will also be standardized, since this is a regression problem - but for PCR that is strictly speaking, not necessary.\n",
    "\n",
    "- Technically, this is a *reduced* SVD, but the full SVD and reduced SVD only differ in the shape of the matrices, not the key ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "\n",
    "def f(x: npt.NDArray[np.float64], w: npt.NDArray[np.float64], b: float) -> float:\n",
    "    f_wb: float = np.dot(w, x) + b\n",
    "    return f_wb\n",
    "\n",
    "def generate_data(n_samples : int, n_features : int, collinear : bool = False, n_collinear : int = 0, corr_strength : float = 0.9, noise : float = 1.0):\n",
    "    '''\n",
    "    Wrapper to generate data for regression. Same as `LinearRegression.ipynb`\n",
    "    '''\n",
    "    X, y, coef = sklearn.datasets.make_regression(n_samples = n_samples, n_features=n_features,\n",
    "                                 n_informative=n_features - (collinear*n_collinear), n_targets=1, \n",
    "                                 bias=2.0, effective_rank=n_features - (collinear*n_collinear),\n",
    "                                 noise=noise, shuffle=True, random_state=42, coef=True)\n",
    "    y : npt.NDArray[np.float64] = y.reshape(-1, 1)\n",
    "    \n",
    "    if (collinear==True):\n",
    "        for i in range(n_features - n_collinear, n_features):\n",
    "            base_feature = np.random.randint(0, n_features - n_collinear)\n",
    "            X[:, i] = corr_strength * X[:, base_feature] + (1 - corr_strength) * np.random.randn(n_samples) * noise ###\n",
    "            \n",
    "    return X, y, coef\n",
    "\n",
    "def normal_solution(X: npt.NDArray[np.float64], y: npt.NDArray[np.float64]) -> npt.NDArray[np.float64]:\n",
    "    '''Find solution of regression by normal equations'''\n",
    "    beta : npt.NDArray[np.float64] = np.matmul(np.matmul(np.linalg.inv(np.matmul(X.T, X)), X.T), y)\n",
    "    return beta\n",
    "\n",
    "def standardize(X : npt.NDArray[np.float64], y : npt.NDArray[np.float64], train_set : bool = False, params : tuple[npt.NDArray[np.float64], npt.NDArray[np.float64], npt.NDArray[np.float64], npt.NDArray[np.float64]] = None) -> tuple[npt.NDArray[np.float64], npt.NDArray[np.float64], tuple[npt.NDArray[np.float64], npt.NDArray[np.float64], npt.NDArray[np.float64], npt.NDArray[np.float64]]]:\n",
    "    '''Standardize Dataset'''\n",
    "    if (train_set == True) and (params is None):\n",
    "        mu_X : npt.NDArray[np.float64] = np.mean(X, axis=0)\n",
    "        mu_y : npt.NDArray[np.float64] = np.mean(y)\n",
    "        std_X : npt.NDArray[np.float64] = np.std(X, axis=0)\n",
    "        std_y : npt.NDArray[np.float64] = np.std(y, axis=0)\n",
    "\n",
    "        X : npt.NDArray[np.float64] = (X - mu_X)/std_X\n",
    "        y : npt.NDArray[np.float64] = (y - mu_y)/std_y\n",
    "        params : tuple[npt.NDArray[np.float64], npt.NDArray[np.float64], npt.NDArray[np.float64], npt.NDArray[np.float64]] = (mu_X, mu_y, std_X, std_y)\n",
    "        \n",
    "    elif (train_set == False) and (params is not None):\n",
    "        mu_X, mu_y, std_X, std_y = params\n",
    "        X : npt.NDArray[np.float64] = (X - mu_X)/std_X\n",
    "        y : npt.NDArray[np.float64] = (y - mu_y)/std_y\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid set of inputs! Please ensure `params` is not None for train_set == False\")\n",
    "    \n",
    "    return X, y, params\n",
    "\n",
    "def MaxVif(X : npt.NDArray[np.float64]) -> npt.NDArray[np.float64]:\n",
    "    ''' Returns the Maximum VIF amongst all the features'''\n",
    "    N, K = X.shape\n",
    "    vif_i : list[float] = []\n",
    "    for i in range(K):\n",
    "        x_i : npt.NDArray[np.float64] = X[:, i].reshape(-1)\n",
    "        x_rest : npt.NDArray[np.float64] = np.delete(X, i, axis=1)\n",
    "        x_i_pred : npt.NDArray[np.float64] = sklearn.linear_model.LinearRegression().fit(x_rest, x_i).predict(x_rest)\n",
    "        R_i_sq : float = 1 - np.sum(np.power(x_i - x_i_pred, 2.0))/(np.sum(np.power(x_i - np.mean(x_i), 2)))\n",
    "        vif_i.append(1.0/(1.0 - R_i_sq))\n",
    "    vif_i = np.array(vif_i)\n",
    "\n",
    "    return np.array([np.max(vif_i), np.argmax(vif_i)])\n",
    "\n",
    "def PerformanceSummary(y : npt.NDArray[np.float64], y_pred : npt.NDArray[np.float64]) -> dict[str, float]:\n",
    "    y_bar : float = np.mean(y)\n",
    "    mse_f : float = np.sum(np.power(y - y_pred, 2.0))/len(y)\n",
    "    mae_f : float = np.sum(np.absolute(y - y_pred))/len(y)\n",
    "    rsq : float = 1 - (np.sum(np.power((y - y_pred), 2.0)))/(np.sum(np.power((y - y_bar), 2.0)))\n",
    "    perf : dict[str, float] = {\"MSE\":mse_f, \"MAE\": mae_f, \"R^2\": rsq}\n",
    "\n",
    "    return perf\n",
    "\n",
    "def generate_datasets(n_samples : int , n_train : int, n_features : int = 10, collinear : bool = True, n_collinear : int = 2, corr_strength : float = 0.6, noise : float = 2.0) -> tuple[npt.NDArray[np.float64], npt.NDArray[np.float64], npt.NDArray[np.float64], npt.NDArray[np.float64]]:\n",
    "\n",
    "    X, y, _ = generate_data(n_samples, n_features = n_features, collinear=collinear, n_collinear=n_collinear, corr_strength=corr_strength, noise = noise)\n",
    "\n",
    "    X_train : npt.NDArray[np.float64] = X[:n_train]\n",
    "    y_train : npt.NDArray[np.float64] = y[:n_train]\n",
    "    X_train, y_train, params = standardize(X_train, y_train, train_set = True, params=None)\n",
    "\n",
    "    X_test : npt.NDArray[np.float64] = X[n_train:]\n",
    "    y_test : npt.NDArray[np.float64] = y[n_train:]\n",
    "    X_test, y_test, params = standardize(X_test, y_test, train_set = False, params=params)\n",
    "\n",
    "    max_vif, max_vif_idx = MaxVif(X_train)\n",
    "\n",
    "    if (max_vif < 5):\n",
    "        print(f\"Max VIF: {max_vif.round(2)} at column: {int(max_vif_idx)}\")\n",
    "        print(\"Maximum VIF in training set < 5, no need to deal with multicollinearity\")\n",
    "    else:\n",
    "        print(\"WARNING!\")\n",
    "        print(f\"Max VIF: {max_vif.round(2)} at column: {int(max_vif_idx)}\")\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# solution based on normal equation\n",
    "\n",
    "def NormalEquationSolution(X_train: npt.NDArray[np.float64], y_train : npt.NDArray[np.float64], X_test : npt.NDArray[np.float64], y_test : npt.NDArray[np.float64]):\n",
    "\n",
    "    w_fit : npt.NDArray[np.float64] = normal_solution(X_train, y_train).reshape(-1)\n",
    "    b_fit : float = 0.0\n",
    "    # bias is zero after standardization, no need to fit it.\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(\"Solution based on Normal Equations\")\n",
    "    \n",
    "    # performance metrics\n",
    "    y_pred : npt.NDArray[np.float64] = np.zeros(y_test.shape)\n",
    "    for i in range(len(y_pred)):\n",
    "        y_pred[i] = f(X_test[i], w_fit, b_fit)\n",
    "\n",
    "    perf : dict[str, float] = PerformanceSummary(y_test, y_pred)\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(f\"MSE after training (test set): {perf['MSE'].round(3)}\")\n",
    "    print(f\"MAE after training (test set): {perf['MAE'].round(3)}\")\n",
    "    print(f\"R^2 after training (test set): {perf['R^2'].round(3)}\")\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max VIF: 1.17 at column: 0\n",
      "Maximum VIF in training set < 5, no need to deal with multicollinearity\n",
      "\n",
      "\n",
      "SOLUTION BASED ON LINEAR REGRESSION\n",
      "--------------------------------------------------------\n",
      "Solution based on Normal Equations\n",
      "--------------------------------------------------------\n",
      "MSE after training (test set): 0.003\n",
      "MAE after training (test set): 0.045\n",
      "R^2 after training (test set): 0.994\n",
      "\n",
      "\n",
      "SOLUTION BASED ON PRINCIPAL COMPONENT REGRESSION\n",
      "--------------------------------------------------------\n",
      "Solution based on Normal Equations\n",
      "--------------------------------------------------------\n",
      "MSE after training (test set): 0.003\n",
      "MAE after training (test set): 0.045\n",
      "R^2 after training (test set): 0.994\n"
     ]
    }
   ],
   "source": [
    "# no collinearity - same performance\n",
    "\n",
    "n_samples = 100\n",
    "n_train = int(0.8 * n_samples)\n",
    "n_features = 10\n",
    "n_collinear = 0\n",
    "n_useful_PCA = n_features - n_collinear \n",
    "\n",
    "X_train, y_train, X_test, y_test = generate_datasets(n_samples, n_train, n_features = n_features,\n",
    "                                                      collinear = False, n_collinear = n_collinear, corr_strength = 0.0, \n",
    "                                                      noise = 1.0)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"SOLUTION BASED ON LINEAR REGRESSION\")\n",
    "NormalEquationSolution(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Principal Components Regression\n",
    "print()\n",
    "print()\n",
    "print(\"SOLUTION BASED ON PRINCIPAL COMPONENT REGRESSION\")\n",
    "V_train = np.linalg.svd(X_train).Vh.T\n",
    "Z_train = np.matmul(X_train, V_train)\n",
    "Z_test = np.matmul(X_test, V_train)\n",
    "\n",
    "# only first n_useful_PCA features have predictive power - the rest are correlated to them and noisy and we can perform \"dimensionality reduction\" by removing them.\n",
    "NormalEquationSolution(Z_train[:, :n_useful_PCA], y_train, Z_test[:, :n_useful_PCA], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING!\n",
      "Max VIF: 36.19 at column: 7\n",
      "\n",
      "\n",
      "SOLUTION BASED ON LINEAR REGRESSION\n",
      "--------------------------------------------------------\n",
      "Solution based on Normal Equations\n",
      "--------------------------------------------------------\n",
      "MSE after training (test set): 0.317\n",
      "MAE after training (test set): 0.497\n",
      "R^2 after training (test set): 0.312\n",
      "\n",
      "\n",
      "SOLUTION BASED ON PRINCIPAL COMPONENT REGRESSION\n",
      "--------------------------------------------------------\n",
      "Solution based on Normal Equations\n",
      "--------------------------------------------------------\n",
      "MSE after training (test set): 0.299\n",
      "MAE after training (test set): 0.48\n",
      "R^2 after training (test set): 0.351\n"
     ]
    }
   ],
   "source": [
    "# some collinearity - PCR performs slightly better\n",
    "\n",
    "n_samples = 100\n",
    "n_train = int(0.8 * n_samples)\n",
    "n_features = 10\n",
    "n_collinear = 3\n",
    "n_useful_PCA = n_features - n_collinear \n",
    "\n",
    "X_train, y_train, X_test, y_test = generate_datasets(n_samples, n_train, n_features = n_features,\n",
    "                                                      collinear = True, n_collinear = n_collinear, corr_strength = 0.85, \n",
    "                                                      noise = 0.1)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"SOLUTION BASED ON LINEAR REGRESSION\")\n",
    "NormalEquationSolution(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Principal Components Regression\n",
    "print()\n",
    "print()\n",
    "print(\"SOLUTION BASED ON PRINCIPAL COMPONENT REGRESSION\")\n",
    "V_train = np.linalg.svd(X_train).Vh.T\n",
    "Z_train = np.matmul(X_train, V_train)\n",
    "Z_test = np.matmul(X_test, V_train)\n",
    "\n",
    "# only first n_useful_PCA features have predictive power - the rest are correlated to them and noisy and we can perform \"dimensionality reduction\" by removing them.\n",
    "NormalEquationSolution(Z_train[:, :n_useful_PCA], y_train, Z_test[:, :n_useful_PCA], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING!\n",
      "Max VIF: inf at column: 0\n",
      "\n",
      "\n",
      "SOLUTION BASED ON LINEAR REGRESSION\n",
      "Could not solve: Singular Matrix\n",
      "\n",
      "\n",
      "SOLUTION BASED ON PRINCIPAL COMPONENT REGRESSION\n",
      "--------------------------------------------------------\n",
      "Solution based on Normal Equations\n",
      "--------------------------------------------------------\n",
      "MSE after training (test set): 0.31\n",
      "MAE after training (test set): 0.491\n",
      "R^2 after training (test set): 0.328\n"
     ]
    }
   ],
   "source": [
    "# very high collinearity - PCR works, while Normal Equation solution of ordinary least squares fails\n",
    "\n",
    "n_samples = 100\n",
    "n_train = int(0.8 * n_samples)\n",
    "n_features = 10\n",
    "n_collinear = 3\n",
    "n_useful_PCA = n_features - n_collinear \n",
    "\n",
    "X_train, y_train, X_test, y_test = generate_datasets(n_samples, n_train, n_features = n_features,\n",
    "                                                      collinear = True, n_collinear = n_collinear, corr_strength = 1.0, \n",
    "                                                      noise = 0.1)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"SOLUTION BASED ON LINEAR REGRESSION\")\n",
    "try:\n",
    "    NormalEquationSolution(X_train, y_train, X_test, y_test)\n",
    "except np.linalg.LinAlgError:\n",
    "    print(\"Could not solve: Singular Matrix\")\n",
    "\n",
    "# Principal Components Regression\n",
    "print()\n",
    "print()\n",
    "print(\"SOLUTION BASED ON PRINCIPAL COMPONENT REGRESSION\")\n",
    "V_train = np.linalg.svd(X_train, full_matrices=False).Vh.T\n",
    "Z_train = np.matmul(X_train, V_train)\n",
    "Z_test = np.matmul(X_test, V_train)\n",
    "\n",
    "# only first n_useful_PCA features have predictive power - the rest are correlated to them and noisy and we can perform \"dimensionality reduction\" by removing them.\n",
    "NormalEquationSolution(Z_train[:, :n_useful_PCA], y_train, Z_test[:, :n_useful_PCA], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and closing remarks\n",
    "\n",
    "- Although we used normal equations for solving the regression problems above, it should be noted that for PCR, one of the advantages is that since all the features are orthogonal, one could simply use univariate regression repeatedly. This is conceptually and computationally simpler.\n",
    "\n",
    "- One of the key problems with PCR is that despite all our efforts, what we are essentially doing is finding directions in an **unsupervised** manner - and hence the \"importance\" of directions, as dictated by the singular values, may not reflect the importance in terms of label prediction. Indeed, if we just look at the example above - the first feature isn't necessarily the most important one in terms of fitting, and the fit can be **significantly** improved by choosing more vectors.\n",
    "\n",
    "- Nonetheless, we see that PCR works in cases of heavy collinearity, can be used for dimensionality reduction, and also reduces to the ordinary least-squares linear regression case otherwise.\n",
    "\n",
    "- There **is**, however, a computational overhead to performing SVD - it is a computationally expensive algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
