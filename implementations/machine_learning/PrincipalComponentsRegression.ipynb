{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Components Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we implement Principal Components Regression (PCR) from scratch and apply it to a synthetically generated dataset. We will also see the advantages of Principal Components Regression versus ordinary Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, before we begin anything, we import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "\n",
    "# bad practice in general, but useful to declutter output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The main idea behind Principal Components Regression (PCR) and its \"successor\", Partial Least Squares (PLS) is that of finding new directions (or new features) from the given vectors (features). The goal is to find features that would better represent the data, and hence, help us make better predictions.\n",
    "\n",
    "It is based on a method called Principal Components Analysis (PCA), which is primarily a dimensionality reduction technique that works by finding directions (features) where the data has the maximum variance. This helps us pick and choose the \"most important\" features for the data and drop the remaining ones.\n",
    "\n",
    "- Note: in many cases, the feature along which the data has the largest variance is the most important one in terms of predictive power. This, however, is not a strict condition and there are times when it is violated.\n",
    "\n",
    "Moreover, the directions found by PCA are **mutually orthogonal**, and hence PCA has the added advantage of finding directions or features that are **linearly independent** - fixing any collinearity or linear dependence present in the original features. \n",
    "\n",
    "These are the key reasons why we are interested in PCR - not only can we perform dimensionality reduction on our feature set, we can also effectively deal with collinearity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "Let $X$ be the $N \\times p$ input matrix. Perform the **Singular Value Decomposition** (SVD) of $X$ as follows:\n",
    "\n",
    "$$X = U D V^T$$\n",
    "\n",
    "Where $U$ is a $N \\times p$ orthogonal matrix, $D$ is a $p \\times p$ diagonal matrix and $V$ is a $p \\times p$ orthogonal matrix. Here $N$ is the number of examples and $p$ is the number of features per example.\n",
    "\n",
    "The columns of $V$ are called the **Principal Component Directions** of $X$. The transformed dataset, $Z$, can be computed by projecting the dataset $X$ onto the principal component directions as follows:\n",
    "\n",
    "$$ Z = X V$$\n",
    "\n",
    "Now all we need to do is treat $Z$ as the dataset and perform regression with the same labels, $y$. For any future vectors (say, some vector $x$), all we have to do, again, is to project them as:\n",
    "\n",
    "$$ z = x V $$\n",
    "\n",
    "to get the transformed vector.\n",
    "\n",
    "Note that this is assuming we don't do any dimensionality reduction - we could (and in many cases, do), of course, perform trivial dimensionality reduction by truncating the number of columns of $Z$ to whatever we desire, since the columns of $V$ are ordered in terms of decreasing importance when performing SVD (that is, decreasing corresponding singular values).\n",
    "\n",
    "\n",
    "**NOTES** \n",
    "- The inputs $X$ (and the vector $x$) are assumed to be standardized. Most often in practice the labels $y$ will also be standardized, since this is a regression problem - but for PCR that is strictly speaking, not necessary.\n",
    "\n",
    "- Technically, this is a *reduced* SVD, but the full SVD and reduced SVD only differ in the shape of the matrices, not the key ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCR():\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        self._mu_X  = None\n",
    "        self._mu_y  = None\n",
    "        self._std_X = None\n",
    "        self._std_y = None\n",
    "\n",
    "        self._V = None\n",
    "        self._num_ignored_features = None\n",
    "        self._theta = None\n",
    "\n",
    "        return None\n",
    "    \n",
    "    def _compute_statistics(self, X, y):\n",
    "        \n",
    "        self._mu_X  = np.mean(X, axis=0)\n",
    "        self._mu_y  = np.mean(y)\n",
    "        self._std_X = np.std(X, axis=0)\n",
    "        self._std_y = np.std(y, axis=0)\n",
    "\n",
    "        return None\n",
    "    \n",
    "    def _standardize(self, X, y):\n",
    "        \n",
    "        X_standardized = (X - self._mu_X)/self._std_X\n",
    "        y_standardized = (y - self._mu_y)/self._std_y\n",
    "        \n",
    "        return X_standardized, y_standardized\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "\n",
    "        self._compute_statistics(X, y)\n",
    "        X0, y0 = self._standardize(X, y)\n",
    "\n",
    "        N, p = X0.shape\n",
    "        num_ignored_features : int = 0\n",
    "\n",
    "        svd = np.linalg.svd(X0, full_matrices=False)\n",
    "        V = svd.Vh.T\n",
    "        S = svd.S\n",
    "\n",
    "        num_singular_vals = S.shape[0]\n",
    "        num_ignored_features = p - num_singular_vals\n",
    "\n",
    "\n",
    "        for j in range(num_singular_vals):\n",
    "            if (np.abs(S[j]) < 1e-6):\n",
    "                S = S[:j]\n",
    "                V = V[:, :j]\n",
    "                num_ignored_features += (num_singular_vals - j)\n",
    "                break\n",
    "        \n",
    "        self._V = V\n",
    "        self._num_ignored_features = num_ignored_features\n",
    "\n",
    "        Z0 = np.matmul(X0, self._V)\n",
    "        theta = np.matmul(np.matmul(np.linalg.inv(np.matmul(Z0.T, Z0)), Z0.T), y0)\n",
    "\n",
    "        self._theta = theta\n",
    "\n",
    "        return None\n",
    "    \n",
    "    def predict(self, X):\n",
    "\n",
    "        X0 = (X - self._mu_X)/self._std_X\n",
    "        Z0 = np.matmul(X0, self._V)\n",
    "        yhat = (np.matmul(Z0, self._theta) * self._std_y) + self._mu_y\n",
    "        \n",
    "        return yhat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions - mainly used for generating helpful information and performance summaries\n",
    "# Also used for working with Linear Regression (Normal Equations method).\n",
    "\n",
    "def generate_data(n_samples : int, n_features : int, collinear : bool = False, n_collinear : int = 0, corr_strength : float = 0.9, noise : float = 1.0):\n",
    "    '''\n",
    "    Wrapper to generate data for regression. Same as `LinearRegression.ipynb`\n",
    "    '''\n",
    "    X, y, coef = sklearn.datasets.make_regression(n_samples = n_samples, n_features=n_features,\n",
    "                                 n_informative=n_features - (collinear*n_collinear), n_targets=1, \n",
    "                                 bias=2.0, effective_rank=n_features - (collinear*n_collinear),\n",
    "                                 noise=noise, shuffle=True, random_state=42, coef=True)\n",
    "    y : npt.NDArray[np.float64] = y.reshape(-1, 1)\n",
    "    \n",
    "    if (collinear==True):\n",
    "        for i in range(n_features - n_collinear, n_features):\n",
    "            base_feature = np.random.randint(0, n_features - n_collinear)\n",
    "            X[:, i] = corr_strength * X[:, base_feature] + (1 - corr_strength) * np.random.randn(n_samples) * noise\n",
    "            \n",
    "    return X, y, coef\n",
    "\n",
    "def MaxVif(X : npt.NDArray[np.float64]) -> npt.NDArray[np.float64]:\n",
    "    ''' Returns the Maximum VIF amongst all the features'''\n",
    "    N, K = X.shape\n",
    "    vif_i : list[float] = []\n",
    "    for i in range(K):\n",
    "        x_i : npt.NDArray[np.float64] = X[:, i].reshape(-1)\n",
    "        x_rest : npt.NDArray[np.float64] = np.delete(X, i, axis=1)\n",
    "        x_i_pred : npt.NDArray[np.float64] = sklearn.linear_model.LinearRegression().fit(x_rest, x_i).predict(x_rest)\n",
    "        R_i_sq : float = 1 - np.sum(np.power(x_i - x_i_pred, 2.0))/(np.sum(np.power(x_i - np.mean(x_i), 2)))\n",
    "        vif_i.append(1.0/(1.0 - R_i_sq))\n",
    "    vif_i = np.array(vif_i)\n",
    "\n",
    "    return np.array([np.max(vif_i), np.argmax(vif_i)])\n",
    "\n",
    "def PerformanceSummary(y : npt.NDArray[np.float64], y_pred : npt.NDArray[np.float64]) -> dict[str, float]:\n",
    "    y_bar : float = np.mean(y)\n",
    "    mse_f : float = np.sum(np.power(y - y_pred, 2.0))/len(y)\n",
    "    mae_f : float = np.sum(np.absolute(y - y_pred))/len(y)\n",
    "    rsq : float = 1 - (np.sum(np.power((y - y_pred), 2.0)))/(np.sum(np.power((y - y_bar), 2.0)))\n",
    "    perf : dict[str, float] = {\"MSE\":mse_f, \"MAE\": mae_f, \"R^2\": rsq}\n",
    "\n",
    "    return perf\n",
    "\n",
    "def generate_datasets(n_samples : int , n_train : int, n_features : int = 10, collinear : bool = True, n_collinear : int = 2, corr_strength : float = 0.6, noise : float = 2.0) -> tuple[npt.NDArray[np.float64], npt.NDArray[np.float64], npt.NDArray[np.float64], npt.NDArray[np.float64]]:\n",
    "\n",
    "    def standardize(X : npt.NDArray[np.float64], y : npt.NDArray[np.float64], train_set : bool = False, params : tuple[npt.NDArray[np.float64], npt.NDArray[np.float64], npt.NDArray[np.float64], npt.NDArray[np.float64]] = None) -> tuple[npt.NDArray[np.float64], npt.NDArray[np.float64], tuple[npt.NDArray[np.float64], npt.NDArray[np.float64], npt.NDArray[np.float64], npt.NDArray[np.float64]]]:\n",
    "        '''Standardize Dataset - a function brought over for linear regression. Defined within `generate_datasets` to avoid confusion and conflict with PCR class.'''\n",
    "        if (train_set == True) and (params is None):\n",
    "            mu_X : npt.NDArray[np.float64] = np.mean(X, axis=0)\n",
    "            mu_y : npt.NDArray[np.float64] = np.mean(y)\n",
    "            std_X : npt.NDArray[np.float64] = np.std(X, axis=0)\n",
    "            std_y : npt.NDArray[np.float64] = np.std(y, axis=0)\n",
    "\n",
    "            X : npt.NDArray[np.float64] = (X - mu_X)/std_X\n",
    "            y : npt.NDArray[np.float64] = (y - mu_y)/std_y\n",
    "            params : tuple[npt.NDArray[np.float64], npt.NDArray[np.float64], npt.NDArray[np.float64], npt.NDArray[np.float64]] = (mu_X, mu_y, std_X, std_y)\n",
    "            \n",
    "        elif (train_set == False) and (params is not None):\n",
    "            mu_X, mu_y, std_X, std_y = params\n",
    "            X : npt.NDArray[np.float64] = (X - mu_X)/std_X\n",
    "            y : npt.NDArray[np.float64] = (y - mu_y)/std_y\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid set of inputs! Please ensure `params` is not None for train_set == False\")\n",
    "        \n",
    "        return X, y, params\n",
    "\n",
    "    X, y, _ = generate_data(n_samples, n_features = n_features, collinear=collinear, n_collinear=n_collinear, corr_strength=corr_strength, noise = noise)\n",
    "\n",
    "    X_train : npt.NDArray[np.float64] = X[:n_train]\n",
    "    y_train : npt.NDArray[np.float64] = y[:n_train]\n",
    "    X_train, y_train, params = standardize(X_train, y_train, train_set = True, params=None)\n",
    "\n",
    "    X_test : npt.NDArray[np.float64] = X[n_train:]\n",
    "    y_test : npt.NDArray[np.float64] = y[n_train:]\n",
    "    X_test, y_test, params = standardize(X_test, y_test, train_set = False, params=params)\n",
    "\n",
    "    max_vif, max_vif_idx = MaxVif(X_train)\n",
    "\n",
    "    if (max_vif < 5):\n",
    "        print(f\"Max VIF: {max_vif.round(2)} at column: {int(max_vif_idx)}\")\n",
    "        print(\"Maximum VIF in training set < 5, no need to deal with multicollinearity\")\n",
    "    else:\n",
    "        print(\"WARNING!\")\n",
    "        print(f\"Max VIF: {max_vif.round(2)} at column: {int(max_vif_idx)}\")\n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalEquationSolution(X_train: npt.NDArray[np.float64], y_train : npt.NDArray[np.float64], X_test : npt.NDArray[np.float64], y_test : npt.NDArray[np.float64]):\n",
    "\n",
    "    def f(x: npt.NDArray[np.float64], w: npt.NDArray[np.float64], b: float) -> float:\n",
    "        ''' Linear Regression equation - local function '''\n",
    "        f_wb: float = np.dot(w, x) + b\n",
    "        return f_wb\n",
    "\n",
    "    def normal_solution(X: npt.NDArray[np.float64], y: npt.NDArray[np.float64]) -> npt.NDArray[np.float64]:\n",
    "        '''Find solution of regression by normal equations - local function'''\n",
    "        beta : npt.NDArray[np.float64] = np.matmul(np.matmul(np.linalg.inv(np.matmul(X.T, X)), X.T), y)\n",
    "        return beta\n",
    "    \n",
    "    w_fit : npt.NDArray[np.float64] = normal_solution(X_train, y_train).reshape(-1)\n",
    "    b_fit : float = 0.0\n",
    "    \n",
    "    y_pred : npt.NDArray[np.float64] = np.zeros(y_test.shape)\n",
    "    for i in range(len(y_pred)):\n",
    "        y_pred[i] = f(X_test[i], w_fit, b_fit)\n",
    "\n",
    "    perf : dict[str, float] = PerformanceSummary(y_test, y_pred)\n",
    "    \n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(\"Solution based on Normal Equations (Ordinary Least Squares)\")\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(f\"MSE after training (test set): {perf['MSE'].round(3)}\")\n",
    "    print(f\"MAE after training (test set): {perf['MAE'].round(3)}\")\n",
    "    print(f\"R^2 after training (test set): {perf['R^2'].round(3)}\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrincipalComponentsRegressionSolution(X_train: npt.NDArray[np.float64], y_train : npt.NDArray[np.float64], X_test : npt.NDArray[np.float64], y_test : npt.NDArray[np.float64]):\n",
    "\n",
    "    PCRRegressor = PCR()\n",
    "    PCRRegressor.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = PCRRegressor.predict(X_test)\n",
    "    perf : dict[str, float] = PerformanceSummary(y_test, y_pred)\n",
    "\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(\"Solution based on Principal Components Regression\")\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(f\"PCR dropped {PCRRegressor._num_ignored_features} features of {X_train.shape[-1]} features.\")\n",
    "    print(f\"MSE after training (test set): {perf['MSE'].round(3)}\")\n",
    "    print(f\"MAE after training (test set): {perf['MAE'].round(3)}\")\n",
    "    print(f\"R^2 after training (test set): {perf['R^2'].round(3)}\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max VIF: 1.17 at column: 0\n",
      "Maximum VIF in training set < 5, no need to deal with multicollinearity\n",
      "--------------------------------------------------------\n",
      "Solution based on Normal Equations (Ordinary Least Squares)\n",
      "--------------------------------------------------------\n",
      "MSE after training (test set): 0.003\n",
      "MAE after training (test set): 0.045\n",
      "R^2 after training (test set): 0.994\n",
      "--------------------------------------------------------\n",
      "Solution based on Principal Components Regression\n",
      "--------------------------------------------------------\n",
      "PCR dropped 0 features of 10 features.\n",
      "MSE after training (test set): 0.003\n",
      "MAE after training (test set): 0.045\n",
      "R^2 after training (test set): 0.994\n"
     ]
    }
   ],
   "source": [
    "# no collinearity - same performance\n",
    "\n",
    "n_samples = 100\n",
    "n_train = int(0.8 * n_samples)\n",
    "n_features = 10\n",
    "n_collinear = 0\n",
    "n_useful_PCA = n_features - n_collinear \n",
    "\n",
    "X_train, y_train, X_test, y_test = generate_datasets(n_samples, n_train, n_features = n_features,\n",
    "                                                      collinear = False, n_collinear = n_collinear, corr_strength = 0.0, \n",
    "                                                      noise = 1.0)\n",
    "\n",
    "\n",
    "NormalEquationSolution(X_train, y_train, X_test, y_test)\n",
    "PrincipalComponentsRegressionSolution(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING!\n",
      "Max VIF: 50.7 at column: 0\n",
      "--------------------------------------------------------\n",
      "Solution based on Normal Equations (Ordinary Least Squares)\n",
      "--------------------------------------------------------\n",
      "MSE after training (test set): 0.313\n",
      "MAE after training (test set): 0.496\n",
      "R^2 after training (test set): 0.322\n",
      "--------------------------------------------------------\n",
      "Solution based on Principal Components Regression\n",
      "--------------------------------------------------------\n",
      "PCR dropped 0 features of 10 features.\n",
      "MSE after training (test set): 0.313\n",
      "MAE after training (test set): 0.496\n",
      "R^2 after training (test set): 0.322\n"
     ]
    }
   ],
   "source": [
    "# some collinearity - PCR may perform slightly better\n",
    "\n",
    "n_samples = 100\n",
    "n_train = int(0.8 * n_samples)\n",
    "n_features = 10\n",
    "n_collinear = 3\n",
    "n_useful_PCA = n_features - n_collinear \n",
    "\n",
    "X_train, y_train, X_test, y_test = generate_datasets(n_samples, n_train, n_features = n_features,\n",
    "                                                      collinear = True, n_collinear = n_collinear, corr_strength = 0.85, \n",
    "                                                      noise = 0.1)\n",
    "NormalEquationSolution(X_train, y_train, X_test, y_test)\n",
    "PrincipalComponentsRegressionSolution(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING!\n",
      "Max VIF: inf at column: 2\n",
      "--------------------------------------------------------\n",
      "Could not solve Linear Regression: Singular Matrix\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Solution based on Principal Components Regression\n",
      "--------------------------------------------------------\n",
      "PCR dropped 3 features of 10 features.\n",
      "MSE after training (test set): 0.31\n",
      "MAE after training (test set): 0.491\n",
      "R^2 after training (test set): 0.328\n"
     ]
    }
   ],
   "source": [
    "# very high collinearity - PCR works, while Normal Equation solution of ordinary least squares fails\n",
    "\n",
    "n_samples = 100\n",
    "n_train = int(0.8 * n_samples)\n",
    "n_features = 10\n",
    "n_collinear = 3\n",
    "n_useful_PCA = n_features - n_collinear \n",
    "\n",
    "X_train, y_train, X_test, y_test = generate_datasets(n_samples, n_train, n_features = n_features,\n",
    "                                                      collinear = True, n_collinear = n_collinear, corr_strength = 1.0, \n",
    "                                                      noise = 0.1)\n",
    "\n",
    "\n",
    "try:\n",
    "    NormalEquationSolution(X_train, y_train, X_test, y_test)\n",
    "except np.linalg.LinAlgError:\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(\"Could not solve Linear Regression: Singular Matrix\")\n",
    "    print(\"--------------------------------------------------------\")\n",
    "PrincipalComponentsRegressionSolution(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING!\n",
      "Max VIF: inf at column: 0\n",
      "--------------------------------------------------------\n",
      "Solution based on Normal Equations (Ordinary Least Squares)\n",
      "--------------------------------------------------------\n",
      "MSE after training (test set): 474192.715\n",
      "MAE after training (test set): 566.57\n",
      "R^2 after training (test set): -902338.755\n",
      "--------------------------------------------------------\n",
      "Solution based on Principal Components Regression\n",
      "--------------------------------------------------------\n",
      "PCR dropped 421 features of 500 features.\n",
      "MSE after training (test set): 0.519\n",
      "MAE after training (test set): 0.585\n",
      "R^2 after training (test set): 0.013\n"
     ]
    }
   ],
   "source": [
    "# high dimensionality - PCR works, while Normal Equation solution of ordinary least squares fails\n",
    "\n",
    "n_samples = 100\n",
    "n_train = int(0.8 * n_samples)\n",
    "n_features = 500\n",
    "n_collinear = 3\n",
    "n_useful_PCA = n_features - n_collinear \n",
    "\n",
    "X_train, y_train, X_test, y_test = generate_datasets(n_samples, n_train, n_features = n_features,\n",
    "                                                      collinear = True, n_collinear = n_collinear, corr_strength = 0.7, \n",
    "                                                      noise = 0.1)\n",
    "\n",
    "\n",
    "try:\n",
    "    NormalEquationSolution(X_train, y_train, X_test, y_test)\n",
    "except np.linalg.LinAlgError:\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(\"Could not solve Linear Regression: Singular Matrix\")\n",
    "    print(\"--------------------------------------------------------\")\n",
    "PrincipalComponentsRegressionSolution(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and closing remarks\n",
    "\n",
    "- Although we used normal equations for solving the regression problems above, it should be noted that for PCR, one of the advantages is that since all the features are orthogonal, one could simply use univariate regression repeatedly. This is conceptually and computationally simpler.\n",
    "\n",
    "- One of the key problems with PCR is that despite all our efforts, what we are essentially doing is finding directions in an **unsupervised** manner - and hence the \"importance\" of directions, as dictated by the singular values, may not reflect the importance in terms of label prediction. \n",
    "\n",
    "- Nonetheless, we see that PCR works in cases of heavy collinearity, high dimensionality, and can be used for dimensionality reduction as well. It reduces to the ordinary least-squares linear regression case otherwise.\n",
    "\n",
    "- There **is**, however, a computational overhead to performing SVD - it is a computationally expensive algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
